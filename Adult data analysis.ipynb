{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e686b90",
   "metadata": {},
   "source": [
    "# COMPAS  Data Analysis Notebook\n",
    "\n",
    "In this notebook, we perform the replication of fair kit looking at the ProPublica COMPAS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a618f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "# datasets\n",
    "from aif360.datasets import AdultDataset, BinaryLabelDataset\n",
    "\n",
    "# metrics\n",
    "from fklearn.metric_library import UnifiedMetricLibrary, classifier_quality_score\n",
    "\n",
    "# models\n",
    "from fklearn.scikit_learn_wrapper import LogisticRegression, KNeighborsClassifier, RandomForestClassifier, SVC\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "\n",
    "# pre/post-processing algorithms\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, RejectOptionClassification\n",
    "\n",
    "# model search\n",
    "from fklearn.fair_selection_aif import ModelSearch, DEFAULT_ADB_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c0ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    }
   ],
   "source": [
    "dataset = AdultDataset()\n",
    "# Specific protected group\n",
    "unprivileged = [{'race': 0, 'sex': 0}]\n",
    "privileged = [{'race': 1, 'sex': 1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083ef6f",
   "metadata": {},
   "source": [
    "## Without Fair Kit\n",
    "\n",
    "First, we try to train a fair model without having access to Fair kit. We will do this by\n",
    "1. Naively training a model for accuracy\n",
    "2. Training a model using k-fold cross validation\n",
    "\n",
    "We will do this for Logistic regression, and random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c766bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for calculating Fairness metrics\n",
    "def getFairnessMetrics(metric_library):\n",
    "    #accuracy\n",
    "    acc = metric_library.accuracy_score()\n",
    "\n",
    "    #fairness\n",
    "    fairness_scores = []\n",
    "    fairness_scores.append(acc)\n",
    "    \n",
    "    # equal opportunity difference\n",
    "    eq_opp_diff = metric_library.equal_opportunity_difference()\n",
    "    fairness_scores.append(eq_opp_diff)\n",
    "\n",
    "    # average odds difference\n",
    "    avg_odds_diff = metric_library.average_odds_difference()\n",
    "    fairness_scores.append(avg_odds_diff)\n",
    "\n",
    "    # statistical parity difference\n",
    "    stat_parity_diff = metric_library.statistical_parity_difference()\n",
    "    fairness_scores.append(stat_parity_diff)\n",
    "\n",
    "    # Disperate impact\n",
    "    disperate_impact = metric_library.disparate_impact()\n",
    "    fairness_scores.append(disperate_impact)\n",
    "\n",
    "    return ([\"acc\",\"eq_opp_diff\",\"avg_odds_diff\",\"stat_parity_diff\",\"disperate_impact\"],(fairness_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd83d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the models to use\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data set into training and testing\n",
    "test_frac =0.3\n",
    "dataset_train, dataset_test = dataset.split([1-test_frac], shuffle=False)\n",
    "\n",
    "X = dataset_train.features\n",
    "y_true = dataset_train.labels.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34f9fc",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Training, and data collection of logistic regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a54806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['acc', 'eq_opp_diff', 'avg_odds_diff', 'stat_parity_diff', 'disperate_impact'], [0.8474238962187661, 0.1287867996201329, 0.10605334963882243, 0.20935414755584736, 0.2075361521960095])\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Naive\n",
    "logisticRegressionModel = LogisticRegression(random_state=0,solver = 'liblinear')\n",
    "logisticRegressionModel.fit(X,y_true)\n",
    "\n",
    "naive_predictions = dataset_test.copy()\n",
    "naive_predictions.labels = logisticRegressionModel.predict(dataset_test.features).reshape(-1,1)\n",
    "\n",
    "# Measure Fairness metrics\n",
    "metric_library_naive = UnifiedMetricLibrary(dataset_test,\n",
    "                                            naive_predictions,\n",
    "                                            unprivileged_groups=unprivileged, privileged_groups = privileged)\n",
    "metrics_array = getFairnessMetrics(metric_library_naive)\n",
    "print(metrics_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84581edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['acc', 'eq_opp_diff', 'avg_odds_diff', 'stat_parity_diff', 'disperate_impact'], [0.8497825606250461, 0.11499287749287757, 0.10088933275615063, 0.21477318120850547, 0.21034669167692477])\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Regularized fit\n",
    "logisticRegressionModel = LogisticRegression(random_state=0,solver = 'liblinear', C=0.5)\n",
    "logisticRegressionModel.fit(X,y_true)\n",
    "\n",
    "\n",
    "regularized_predictions = dataset_test.copy()\n",
    "regularized_predictions.labels = logisticRegressionModel.predict(dataset_test.features).reshape(-1,1)\n",
    "\n",
    "# Measure Fairness metrics\n",
    "metric_library_regularized= UnifiedMetricLibrary(dataset_test,\n",
    "                                            regularized_predictions,\n",
    "                                            unprivileged_groups=unprivileged, privileged_groups = privileged)\n",
    "metrics_array = getFairnessMetrics(metric_library_regularized)\n",
    "print(metrics_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04fd013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['acc', 'eq_opp_diff', 'avg_odds_diff', 'stat_parity_diff', 'disperate_impact'], [0.8472027714306773, 0.11263057929724596, 0.09806644706585033, 0.20803839809532976, 0.21214726725061325])\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Cross Validate fit\n",
    "from sklearn.model_selection import cross_validate\n",
    "logisticRegressionModel = LogisticRegression(random_state=0,solver = 'liblinear', C=0.5)\n",
    "cv_results = cross_validate(logisticRegressionModel,X,y_true,cv = 10,return_estimator = True)\n",
    "\n",
    "cv_results['test_score']\n",
    "cv_model = cv_results['estimator'][list(cv_results['test_score']).index(max(cv_results['test_score']))]\n",
    "\n",
    "cv_predictions = dataset_test.copy()\n",
    "cv_predictions.labels = cv_model.predict(dataset_test.features).reshape(-1,1)\n",
    "\n",
    "# Measure Fairness metrics\n",
    "metric_library_cv= UnifiedMetricLibrary(dataset_test,\n",
    "                                            cv_predictions,\n",
    "                                            unprivileged_groups=unprivileged, privileged_groups = privileged)\n",
    "metrics_array = getFairnessMetrics(metric_library_cv)\n",
    "print(metrics_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b4d5d",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Training, and data collection of random Forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961b641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['acc', 'eq_opp_diff', 'avg_odds_diff', 'stat_parity_diff', 'disperate_impact'], [0.8392422790594826, 0.08885327635327633, 0.10048984527112734, 0.23076648274025452, 0.20849253116193067])\n"
     ]
    }
   ],
   "source": [
    "# Random forest Naive\n",
    "RandomForestModel = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "RandomForestModel.fit(X,y_true)\n",
    "\n",
    "naive_predictions = dataset_test.copy()\n",
    "naive_predictions.labels = RandomForestModel.predict(dataset_test.features).reshape(-1,1)\n",
    "\n",
    "# Measure Fairness metrics\n",
    "metric_library = UnifiedMetricLibrary(dataset_test,\n",
    "                                            naive_predictions,\n",
    "                                            unprivileged_groups=unprivileged, privileged_groups = privileged)\n",
    "metrics_array = getFairnessMetrics(metric_library)\n",
    "print(metrics_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a21979b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['acc', 'eq_opp_diff', 'avg_odds_diff', 'stat_parity_diff', 'disperate_impact'], [0.8548684307510872, 0.19944207027540362, 0.12607319352875282, 0.18564969502896775, 0.19194844330787839])\n"
     ]
    }
   ],
   "source": [
    "# Random forest Regularized\n",
    "RandomForestModel = RandomForestClassifier(random_state=0,n_estimators=100,max_depth=10)\n",
    "RandomForestModel.fit(X,y_true)\n",
    "\n",
    "predictions = dataset_test.copy()\n",
    "predictions.labels = RandomForestModel.predict(dataset_test.features).reshape(-1,1)\n",
    "\n",
    "# Measure Fairness metrics\n",
    "metric_library = UnifiedMetricLibrary(dataset_test,\n",
    "                                            predictions,\n",
    "                                            unprivileged_groups=unprivileged, privileged_groups = privileged)\n",
    "metrics_array = getFairnessMetrics(metric_library)\n",
    "print(metrics_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd446bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['acc', 'eq_opp_diff', 'avg_odds_diff', 'stat_parity_diff', 'disperate_impact'], [0.8377681138055576, 0.13346391263057922, 0.12380598859570557, 0.23553406319317466, 0.1921401757766812])\n"
     ]
    }
   ],
   "source": [
    "# Random forest CV\n",
    "RandomForestModel = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "cv_results = cross_validate(RandomForestModel,X,y_true,cv = 10,return_estimator = True)\n",
    "\n",
    "cv_model = cv_results['estimator'][list(cv_results['test_score']).index(max(cv_results['test_score']))]\n",
    "\n",
    "cv_predictions = dataset_test.copy()\n",
    "cv_predictions.labels = cv_model.predict(dataset_test.features).reshape(-1,1)\n",
    "\n",
    "# Measure Fairness metrics\n",
    "metric_library = UnifiedMetricLibrary(dataset_test,\n",
    "                                            cv_predictions,\n",
    "                                            unprivileged_groups=unprivileged, privileged_groups = privileged)\n",
    "metrics_array = getFairnessMetrics(metric_library)\n",
    "print(metrics_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573977a",
   "metadata": {},
   "source": [
    "## With Fairkit\n",
    "\n",
    "Using fair kit to find the most fair model. The data generation part of this is also included in the \"compasGridSearch.py\" script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0436fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    }
   ],
   "source": [
    "from fklearn.scikit_learn_wrapper import LogisticRegression, KNeighborsClassifier, RandomForestClassifier, SVC\n",
    "## Re setup data set (sanity check)\n",
    "dataset = AdultDataset()\n",
    "# Specific protected group\n",
    "unprivileged = [{'race': 0, 'sex': 0}]\n",
    "privileged = [{'race': 1, 'sex': 1}]\n",
    "\n",
    "## Setup parameters\n",
    "\n",
    "models = {'LogisticRegression': LogisticRegression, 'RandomForestClassifier': RandomForestClassifier }\n",
    "\n",
    "metrics = {'UnifiedMetricLibrary': [UnifiedMetricLibrary,\n",
    "                                    'accuracy_score',\n",
    "                                    'average_odds_difference',\n",
    "                                    'statistical_parity_difference',\n",
    "                                    'equal_opportunity_difference',\n",
    "                                    'disparate_impact'\n",
    "                                   ]\n",
    "          }\n",
    "\n",
    "hyperparameters = {'LogisticRegression':{'penalty': ['l1', 'l2'], 'C': [0.1, 0.5, 1, 1.5],'solver':['liblinear']},\n",
    "                   'RandomForestClassifier':{'n_estimators': ['warn', 10, 20, 30, 40, 50, 100]\n",
    "                  }}\n",
    "\n",
    "thresholds = [i * 10.0/100 for i in range(5)]\n",
    "\n",
    "processor_args = {'unprivileged_groups': unprivileged, 'privileged_groups': privileged}\n",
    "\n",
    "preprocessors=[DisparateImpactRemover()]\n",
    "postprocessors=[CalibratedEqOddsPostprocessing(**processor_args)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b1026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## Perform Grid search.\n",
    "Search = ModelSearch(models, metrics, hyperparameters, thresholds) \n",
    "Search.grid_search(dataset, privileged=privileged, unprivileged=unprivileged, preprocessors=preprocessors, postprocessors=postprocessors)\n",
    "\n",
    "Search.to_csv(\"adult_search_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22528303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for visualization\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "from bokeh.application import Application\n",
    "\n",
    "# load Bokeh\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fklearn.interface.plot import *\n",
    "import os, fklearn\n",
    "\n",
    "# Define function that takes in a document and attaches the bokeh server to it\n",
    "def modify_doc(doc):\n",
    "    \n",
    "    # Load custom styles (for notebook only)\n",
    "    style = os.path.join(fklearn.__path__[0], 'interface', 'static', 'css', 'styles-notebook.css')\n",
    "    custom_css = Div(text=\"<link rel='stylesheet' type='text/css' href=\" + style + \">\")\n",
    "    add_btn = Button(label=\"Add Plot\", button_type=\"success\")\n",
    "    remove_btn = Button(label=\"Remove Plot\", button_type=\"danger\")\n",
    "\n",
    "    explanations = os.path.join(fklearn.__path__[0], 'interface', 'static', 'data', 'explanations.json')\n",
    "    \n",
    "    # Construct our viewport\n",
    "    l = layout([\n",
    "        [custom_css],\n",
    "        create_plot(\"adult_search_output.csv\", explanations)\n",
    "    ], sizing_mode=\"fixed\", css_classes=[\"layout-container\"])\n",
    "\n",
    "    doc.add_root(l)\n",
    "    \n",
    "# Set up the application\n",
    "handler = FunctionHandler(modify_doc)\n",
    "app = Application(handler)\n",
    "\n",
    "# Render visualization in the notebook\n",
    "show(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57205411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08ad20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
